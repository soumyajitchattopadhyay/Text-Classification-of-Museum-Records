# -*- coding: utf-8 -*-
"""Coursework1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ghKF1ySE1KrZ1PmWWMzuN1VyRg-IKh_
"""

from google.colab import files

#Upload multiple files
uploaded = files.upload()

import json
import pandas as pd


file_path = "dataset.json"
with open(file_path, "r", encoding="utf-8") as f:
    data = json.load(f)
train_data = data.get("train", [])
df = pd.DataFrame(train_data)

label_columns = ["labl", "label", "key"]
df["institution"] = df[label_columns].bfill(axis=1).iloc[:, 0]


df.drop(columns=label_columns, inplace=True)

label_corrections = {
    "oyal Botanic Gardens, Kew": "Royal Botanic Gardens, Kew",
    "oyal College of Physicians of London": "Royal College of Physicians of London",
    "hakespeare Birthplace Trust": "Shakespeare Birthplace Trust",
    "ational Railway Museum": "National Railway Museum",
    "ational Maritime Museum": "National Maritime Museum"
}
df["institution"] = df["institution"].replace(label_corrections)

label_mapping = {
    "National Maritime Museum": "0",
    "National Railway Museum": "1",
    "Royal Botanic Gardens, Kew": "2",
    "Royal College of Physicians of London": "3",
    "Shakespeare Birthplace Trust": "4"
}

df["institution"] = df["institution"].replace(label_mapping)
df["text"] = df["text"].fillna(df["description"]).fillna(df["content"])
df.drop(columns=["description", "content"], inplace=True)
df.drop_duplicates(inplace=True)
print("Unique Institution Labels After Cleaning:")
print(df["institution"].unique())

cleaned_file_path = "/content/cleaned_dataset.csv"
df.to_csv(cleaned_file_path, index=False)
print(f"\nCleaned dataset saved at: {cleaned_file_path}")

import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt

file_path = "/content/cleaned_dataset.csv"
df = pd.read_csv(file_path)

#(80-10-10 split)
train_ratio, val_ratio, test_ratio = 0.8, 0.1, 0.1
train_size = int(len(df) * train_ratio)
val_size = int(len(df) * val_ratio)
test_size = len(df) - (train_size + val_size)

df_train = df.iloc[:train_size]
df_val = df.iloc[train_size:train_size + val_size]
df_test = df.iloc[train_size + val_size:]

print(f"Sample counts:")
print(f"- Training Set: {len(df_train)}")
print(f"- Validation Set: {len(df_val)}")
print(f"- Test Set: {len(df_test)}")

total_samples = len(df)
print("\nPercentage splits:")
print(f"- Training: {len(df_train) / total_samples:.2%}")
print(f"- Validation: {len(df_val) / total_samples:.2%}")
print(f"- Test: {len(df_test) / total_samples:.2%}")

print("\nText length analysis (characters):")
for split_name, split_df in zip(["Training", "Validation", "Test"], [df_train, df_val, df_test]):
    text_lengths = split_df["text"].astype(str).apply(len)
    print(f"- {split_name} Set: Min = {text_lengths.min()}, Max = {text_lengths.max()}")

nlp = spacy.load("en_core_web_sm")

def get_top_tokens(texts, top_n=5):
    all_tokens = []
    for text in texts.dropna():
        doc = nlp(text.lower())
        tokens = [token.text for token in doc if token.is_alpha and not token.is_stop]
        all_tokens.extend(tokens)
    return [token for token, _ in Counter(all_tokens).most_common(top_n)]

print("\nMost frequent 5 tokens in each class:")
for institution in df["institution"].unique():
    class_texts = df_train[df_train["institution"] == institution]["text"]
    top_tokens = get_top_tokens(class_texts)
    print(f"- {institution}: {top_tokens}")

df_train.to_csv("/content/train.csv", index=False)
df_val.to_csv("/content/val.csv", index=False)
df_test.to_csv("/content/test.csv", index=False)

print("Unique ground truth labels:", df["institution"].unique())
print(df.columns)
print("Unique institution names:", df["institution"].unique())

import pandas as pd

cleaned_data_path = "/content/cleaned_dataset.csv"
df_gt = pd.read_csv(cleaned_data_path)

df_gt = df_gt.loc[:, ~df_gt.columns.duplicated()]

if "institution" in df_gt.columns:
    df_gt.rename(columns={"institution": "true_label"}, inplace=True)

df_gt["true_label"] = df_gt["true_label"].astype(str)

print("Unique true labels:", df_gt["true_label"].unique())  # Expected: ['0', '1', '2', '3', '4'

import json
import pandas as pd

def load_llm_predictions(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    df_preds = pd.DataFrame(raw_data)

    # Rename 'next_token' to match expected column
    df_preds.rename(columns={"next_token": "predicted_label"}, inplace=True)

    # Ensure predicted_label is string and map invalids to '5'
    valid_labels = {"0", "1", "2", "3", "4"}
    df_preds["predicted_label"] = df_preds["predicted_label"].apply(
        lambda x: str(x) if str(x) in valid_labels else "5"
    )

    return df_preds
preds1 = load_llm_predictions("llm_prompt_template_1.json")
preds2 = load_llm_predictions("llm_prompt_template_2.json")
preds3 = load_llm_predictions("llm_prompt_template_3.json")

# Ensure 'true_label' column exists before merging
df = df.rename(columns={"institution": "true_label"})

df_eval_1 = df.merge(preds1, on="id", how="inner")
df_eval_2 = df.merge(preds2, on="id", how="inner")
df_eval_3 = df.merge(preds3, on="id", how="inner")

# Remove duplicated columns (if any)
df_eval_1 = df_eval_1.loc[:, ~df_eval_1.columns.duplicated()]
df_eval_2 = df_eval_2.loc[:, ~df_eval_2.columns.duplicated()]
df_eval_3 = df_eval_3.loc[:, ~df_eval_3.columns.duplicated()]

# Verify the Fix
print(df_eval_1.columns)
print(df_eval_1.dtypes)
print("Unique values in true_label:", df_eval_1["true_label"].unique())
print("Unique values in predicted_label:", df_eval_1["predicted_label"].unique())

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def load_llm_predictions(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        llm_data = json.load(f)
    df_preds = pd.DataFrame(llm_data)
    df_preds.rename(columns={"next_token": "predicted_label"}, inplace=True)
    valid_labels = {"0", "1", "2", "3", "4"}
    df_preds["predicted_label"] = df_preds["predicted_label"].apply(lambda x: x if str(x) in valid_labels else "5")

    return df_preds

llm_pred_1 = load_llm_predictions("/content/llm_prompt_template_1.json")
llm_pred_2 = load_llm_predictions("/content/llm_prompt_template_2.json")
llm_pred_3 = load_llm_predictions("/content/llm_prompt_template_3.json")

df_eval_1 = df.merge(llm_pred_1, on="id", how="inner")
df_eval_2 = df.merge(llm_pred_2, on="id", how="inner")
df_eval_3 = df.merge(llm_pred_3, on="id", how="inner")
df_eval_1 = df_eval_1.loc[:, ~df_eval_1.columns.duplicated()]
df_eval_2 = df_eval_2.loc[:, ~df_eval_2.columns.duplicated()]
df_eval_3 = df_eval_3.loc[:, ~df_eval_3.columns.duplicated()]

# Verify the fix
print(df_eval_1.columns)

print(df_eval_1.dtypes)
print("Unique values in true_label:", df_eval_1["true_label"].unique())
print("Unique values in predicted_label:", df_eval_1["predicted_label"].unique())

def compute_metrics(df, template_name):
    df["true_label"] = df["true_label"].astype(str)
    df["predicted_label"] = df["predicted_label"].astype(str)

    accuracy = accuracy_score(df["true_label"], df["predicted_label"])
    precision = precision_score(df["true_label"], df["predicted_label"], average="macro", zero_division=0)
    recall = recall_score(df["true_label"], df["predicted_label"], average="macro", zero_division=0)
    f1 = f1_score(df["true_label"], df["predicted_label"], average="macro", zero_division=0)

    print(f"\nEvaluation for {template_name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Macro Precision: {precision:.4f}")
    print(f"Macro Recall: {recall:.4f}")
    print(f"Macro F1-score: {f1:.4f}")

compute_metrics(df_eval_1, "LLM Prompt Template 1")
compute_metrics(df_eval_2, "LLM Prompt Template 2")
compute_metrics(df_eval_3, "LLM Prompt Template 3")

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
#!pip install datasets

from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Load training and validation datasets
train_path = "/content/train.csv"
val_path = "/content/val.csv"

df_train = pd.read_csv(train_path)
df_val = pd.read_csv(val_path)

# ✅ Fix NaN values and ensure text is a string
df_train["text"] = df_train["text"].fillna("").astype(str)
df_val["text"] = df_val["text"].fillna("").astype(str)

# ✅ Convert labels to string and map them to integers
df_train["true_label"] = df_train["institution"].astype(str)
df_val["true_label"] = df_val["institution"].astype(str)

label_mapping = {label: idx for idx, label in enumerate(df_train["true_label"].unique())}
df_train["label"] = df_train["true_label"].map(label_mapping)
df_val["label"] = df_val["true_label"].map(label_mapping)

# ✅ Load tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# ✅ Define tokenization function
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# ✅ Convert Pandas DataFrame to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(df_train[["text", "label"]])
val_dataset = Dataset.from_pandas(df_val[["text", "label"]])

# ✅ Tokenize datasets
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# ✅ Set format for PyTorch
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# Check for CUDA availability
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using {device} device")

# ✅ Load pre-trained BERT model for classification
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(label_mapping))
# Move model to CUDA device
model = model.to(device)

# ✅ Define training arguments with CUDA optimizations
training_args = TrainingArguments(
    output_dir="/content/CASE1/results",
    num_train_epochs=8,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=5e-5,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="/content/CASE1/logs",
    logging_steps=50,
    # CUDA-specific settings
    fp16=True,  # Mixed precision training (faster on modern GPUs)
    no_cuda=False,  # Ensure CUDA is used if available
)

# ✅ Define evaluation function
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = predictions.argmax(axis=1)
    acc = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="macro", zero_division=0)
    return {"accuracy": acc, "macro_precision": precision, "macro_recall": recall, "macro_f1": f1}

# ✅ Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# ✅ Train the model
trainer.train()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# ✅ Get model predictions
predictions = trainer.predict(val_dataset)
pred_labels = np.argmax(predictions.predictions, axis=1)
true_labels = val_dataset["label"]

# ✅ Compute Confusion Matrix
conf_matrix = confusion_matrix(true_labels, pred_labels)

# ✅ Plot the Confusion Matrix
plt.figure(figsize=(6,5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# ✅ Print Classification Report
print("Classification Report:\n", classification_report(true_labels, pred_labels, target_names=label_mapping.keys()))

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from transformers.integrations import WandbCallback

# ✅ Prepare the training & validation data
train_df = pd.read_csv("/content/train.csv")
val_df = pd.read_csv("/content/val.csv")

# Convert to string and handle NaNs
train_df["text"] = train_df["text"].fillna("").astype(str)
val_df["text"] = val_df["text"].fillna("").astype(str)

# Convert institution to string then map to label IDs
train_df["true_label"] = train_df["institution"].astype(str)
val_df["true_label"] = val_df["institution"].astype(str)

label_mapping = {label: idx for idx, label in enumerate(sorted(train_df["true_label"].unique()))}
train_df["label"] = train_df["true_label"].map(label_mapping)
val_df["label"] = val_df["true_label"].map(label_mapping)

train_ds = Dataset.from_pandas(train_df[["text", "label"]])
val_ds = Dataset.from_pandas(val_df[["text", "label"]])

#  Metric function for trainer
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="macro", zero_division=0)
    acc = accuracy_score(labels, preds)
    return {
        "accuracy": acc,
        "macro_precision": precision,
        "macro_recall": recall,
        "macro_f1": f1
    }
def train_model(base_model_name, output_dir):
    print(f"\n🔧 Training model: {base_model_name}\n")

    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=5)

    # Tokenize the datasets
    def tokenize_fn(batch):
      return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=512)


    tokenized_train = train_ds.map(tokenize_fn, batched=True)
    tokenized_val = val_ds.map(tokenize_fn, batched=True)

    tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    tokenized_val.set_format("torch", columns=["input_ids", "attention_mask", "label"])

    args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=8,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        learning_rate=5e-5,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="macro_f1",
        greater_is_better=True,
        report_to="none",
        save_total_limit=1,
        logging_dir=f"{output_dir}/logs"
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        compute_metrics=compute_metrics
    )

    trainer.train()

    # Final evaluation
    preds = trainer.predict(tokenized_val)
    pred_labels = np.argmax(preds.predictions, axis=1)
    print("\n🧾 Classification Report:\n")
    print(classification_report(tokenized_val["label"], pred_labels, target_names=label_mapping.keys()))

models_to_try = [
    #"bert-base-uncased",
    #"roberta-base",
    #"distilbert-base-uncased",
    #"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract"
]

for model_name in models_to_try:
    output_folder = f"/content/q6_models/{model_name.replace('/', '_')}"
    train_model(model_name, output_folder)

